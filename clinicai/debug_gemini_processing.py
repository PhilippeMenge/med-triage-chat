#!/usr/bin/env python3
"""
Debug especÃ­fico para identificar por que o Gemini estÃ¡ entrando em fallback.
"""

import asyncio
import json
from clinicai_mongodb_fixed import GeminiTriageAgent, TriageSlots, connect_mongodb

async def debug_gemini_processing():
    """Debug detalhado do processamento do Gemini."""
    
    print("ğŸ” DEBUG: Processamento do Gemini")
    print("=" * 60)
    
    await connect_mongodb()
    gemini = GeminiTriageAgent()
    
    if not gemini.client:
        print("âŒ Gemini nÃ£o configurado")
        return
    
    print("âœ… Gemini configurado - iniciando debug detalhado")
    
    # Teste simples
    test_message = "Estou com dor de cabeÃ§a"
    current_slots = TriageSlots()
    conversation_history = []
    
    print(f"\nğŸ“¤ TESTE SIMPLES")
    print(f"   ğŸ‘¤ Mensagem: '{test_message}'")
    print(f"   ğŸ“‹ Slots atuais: {current_slots.model_dump()}")
    print(f"   ğŸ’¬ HistÃ³rico: {conversation_history}")
    
    try:
        print(f"\nğŸ”§ CONFIGURAÃ‡Ã•ES DO GEMINI:")
        print(f"   ğŸ¤– Modelo: {gemini.client.model_name if hasattr(gemini.client, 'model_name') else 'N/A'}")
        print(f"   ğŸ”‘ API Key configurada: {'Sim' if gemini.client else 'NÃ£o'}")
        
        # Testar chamada direta ao Gemini
        print(f"\nğŸ§ª TESTANDO CHAMADA DIRETA AO GEMINI...")
        
        # Construir prompt manualmente como no cÃ³digo
        history_text = ""
        if conversation_history:
            history_text = "\n".join(conversation_history[-6:])
        
        collected_info = {
            "chief_complaint": current_slots.chief_complaint,
            "symptoms": current_slots.symptoms,
            "duration_frequency": current_slots.duration_frequency,
            "intensity": current_slots.intensity,
            "history": current_slots.health_history,
            "measures_taken": current_slots.measures_taken
        }
        
        is_conversation_start = test_message == "[INÃCIO DA CONVERSA]"
        
        if is_conversation_start:
            user_prompt = f"""
CONTEXTO DA CONVERSA:
{history_text}

SITUAÃ‡ÃƒO: Este Ã© o INÃCIO de uma nova triagem. O usuÃ¡rio acabou de receber a mensagem de boas-vindas.

INSTRUÃ‡Ã•ES:
1. FaÃ§a a primeira pergunta para iniciar a coleta de informaÃ§Ãµes
2. Comece perguntando sobre a queixa principal de forma acolhedora
3. Seja empÃ¡tico e profissional
4. Retorne no formato JSON especificado

IMPORTANTE: Esta Ã© a PRIMEIRA pergunta da triagem. Seja acolhedor e direto.
"""
        else:
            user_prompt = f"""
CONTEXTO DA CONVERSA:
{history_text}

INFORMAÃ‡Ã•ES JÃ COLETADAS:
{json.dumps(collected_info, indent=2, ensure_ascii=False)}

NOVA MENSAGEM DO USUÃRIO:
"{test_message}"

INSTRUÃ‡Ã•ES:
1. Analise a mensagem do usuÃ¡rio no contexto da conversa
2. Extraia/atualize informaÃ§Ãµes relevantes para os 6 tÃ³picos da triagem
3. Detecte sinais de emergÃªncia
4. Responda de forma empÃ¡tica e natural
5. Se necessÃ¡rio, faÃ§a uma pergunta para coletar informaÃ§Ã£o faltante
6. Retorne no formato JSON especificado

Se todas as 6 informaÃ§Ãµes estiverem coletadas, marque "is_complete": true e faÃ§a um resumo acolhedor.
"""
        
        print(f"ğŸ“ PROMPT CONSTRUÃDO:")
        print(f"   ğŸ“ Tamanho do prompt: {len(user_prompt)} caracteres")
        print(f"   ğŸ”¤ Primeiras 200 chars: {user_prompt[:200]}...")
        
        # ConfiguraÃ§Ãµes como no cÃ³digo (apenas categorias vÃ¡lidas)
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
        
        generation_config = {
            "temperature": 0.3,
            "max_output_tokens": 400,
            "top_p": 0.8,
            "top_k": 40,
            "candidate_count": 1
        }
        
        print(f"\nâš™ï¸ CONFIGURAÃ‡Ã•ES:")
        print(f"   ğŸŒ¡ï¸ Temperature: {generation_config['temperature']}")
        print(f"   ğŸ“Š Max tokens: {generation_config['max_output_tokens']}")
        print(f"   ğŸ›¡ï¸ Safety settings: {len(safety_settings)} categorias")
        
        # Fazer chamada real
        print(f"\nğŸš€ FAZENDO CHAMADA PARA GEMINI...")
        
        import google.generativeai as genai
        
        full_prompt = f"{gemini._get_system_prompt()}\n\n{user_prompt}"
        print(f"ğŸ“ Prompt completo: {len(full_prompt)} caracteres")
        
        response = await asyncio.to_thread(
            gemini.client.generate_content,
            full_prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        print(f"âœ… RESPOSTA RECEBIDA!")
        print(f"   ğŸ“Š Candidates: {len(response.candidates) if response.candidates else 0}")
        
        if response.candidates:
            candidate = response.candidates[0]
            print(f"   ğŸ¯ Finish reason: {candidate.finish_reason}")
            print(f"   ğŸ›¡ï¸ Safety ratings: {len(candidate.safety_ratings) if candidate.safety_ratings else 0}")
            
            if candidate.safety_ratings:
                for rating in candidate.safety_ratings:
                    print(f"      - {rating.category}: {rating.probability}")
            
            if candidate.content and candidate.content.parts:
                response_text = response.text.strip()
                print(f"   ğŸ“ Texto da resposta: {len(response_text)} caracteres")
                print(f"   ğŸ”¤ Primeiros 200 chars: {response_text[:200]}...")
                
                # Tentar fazer parse do JSON
                try:
                    # Limpar JSON como no cÃ³digo
                    if response_text.startswith("```json"):
                        response_text = response_text.replace("```json", "").replace("```", "").strip()
                    elif response_text.startswith("```"):
                        response_text = response_text.replace("```", "").strip()
                    
                    result = json.loads(response_text)
                    print(f"   âœ… JSON VÃLIDO!")
                    print(f"   ğŸ’¬ Mensagem: {result.get('message', 'N/A')[:100]}...")
                    print(f"   ğŸš¨ EmergÃªncia: {result.get('is_emergency', 'N/A')}")
                    print(f"   âœ… Completo: {result.get('is_complete', 'N/A')}")
                    
                except json.JSONDecodeError as e:
                    print(f"   âŒ ERRO JSON: {e}")
                    print(f"   ğŸ“„ JSON problemÃ¡tico: {response_text}")
                    
            else:
                print(f"   âŒ SEM CONTEÃšDO: Resposta bloqueada")
        else:
            print(f"   âŒ SEM CANDIDATES: Resposta completamente bloqueada")
            
        # Verificar se hÃ¡ prompt feedback
        if hasattr(response, 'prompt_feedback'):
            print(f"   ğŸ“‹ Prompt feedback: {response.prompt_feedback}")
            
    except Exception as e:
        print(f"âŒ ERRO NA CHAMADA DIRETA: {e}")
        print(f"   ğŸ” Tipo do erro: {type(e).__name__}")
        print(f"   ğŸ“„ Detalhes: {str(e)}")
        
        # Verificar se Ã© erro de quota
        if "quota" in str(e).lower():
            print(f"   ğŸ’° PROBLEMA DE QUOTA: API atingiu limite")
        elif "safety" in str(e).lower():
            print(f"   ğŸ›¡ï¸ PROBLEMA DE SEGURANÃ‡A: Filtros bloquearam")
        elif "medical" in str(e).lower():
            print(f"   ğŸ¥ PROBLEMA MÃ‰DICO: Categoria mÃ©dica bloqueada")
    
    print(f"\nğŸ”„ TESTANDO MÃ‰TODO PROCESS_CONVERSATION...")
    
    try:
        result = await gemini.process_conversation(
            user_message=test_message,
            current_slots=current_slots,
            conversation_history=conversation_history
        )
        
        if result:
            print(f"âœ… PROCESS_CONVERSATION FUNCIONOU!")
            print(f"   ğŸ’¬ Mensagem: {result['message'][:100]}...")
        else:
            print(f"âŒ PROCESS_CONVERSATION RETORNOU NONE")
            
    except Exception as e:
        print(f"âŒ ERRO EM PROCESS_CONVERSATION: {e}")

if __name__ == "__main__":
    print("ğŸš€ INICIANDO DEBUG DO PROCESSAMENTO GEMINI")
    
    asyncio.run(debug_gemini_processing())
    
    print(f"\nğŸ¯ DEBUG CONCLUÃDO!")
    print("âœ… AnÃ¡lise detalhada do Gemini realizada")
